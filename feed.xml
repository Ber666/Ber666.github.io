<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://ber666.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ber666.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-11T22:27:31+00:00</updated><id>https://ber666.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal website for Shibo Hao. </subtitle><entry><title type="html">Maximum Entropy RL (1): Soft Q-Learning</title><link href="https://ber666.github.io/blog/2024/soft-RL-01/" rel="alternate" type="text/html" title="Maximum Entropy RL (1): Soft Q-Learning"/><published>2024-08-04T13:56:00+00:00</published><updated>2024-08-04T13:56:00+00:00</updated><id>https://ber666.github.io/blog/2024/soft-RL-01</id><content type="html" xml:base="https://ber666.github.io/blog/2024/soft-RL-01/"><![CDATA[<p>Most deep reinforcement learning (RL) is based on the deterministic notion of optimality, where the optimal solution is always a deterministic policy (at least under full observability). However, a stochastic policy may be preferred due to:</p> <ul> <li><strong>Better exploration</strong>: Strike a balance between exploration and exploitation, in contrast to heuristic exploration methods like $\epsilon$-greedy.</li> <li><strong>Robustness and better generalization</strong>: A stochastic policy is more robust to adversarial perturbations and serves as a better initialization for learning new skills.</li> </ul> <p><strong>Maximum entropy RL</strong> attempts to optimize the entropy of policy in addition to the expected reward, which encourages the policy to be more stochastic. The new learning objective also has some interesting properties and leads to useful algorithms which bridge traditional value and policy based methods (<a href="https://arxiv.org/abs/1702.08165">Soft Q-Learning</a>, <a href="https://arxiv.org/abs/1702.08892">Path Consistency Learning</a>, <a href="https://arxiv.org/abs/1801.01290">Soft Actor-Critic</a>, etc.)</p> <h2 id="functional-derivative">Functional Derivative</h2> <p>In this section, we provide a quick introduction of functional derivative, which is crucial to prove some key theorems in maximum entropy RL.</p> <p><strong>Definition</strong></p> <p>A functional $J(q)$ is a mapping from function $q(x)$ to a real value. Here, we focus on a simple case of functional, which can be expressed as an integration of a function of $q(x)$:</p> \[J[q] = \int_a^b F(q(x), x) \, dx\] <p>To measure <strong>“how a small change in $q(x)$ would affect $J(q)$”</strong>, we define the functinoal derivative $\frac{\delta J}{\delta q(x)}$ as follows:</p> \[J[q + \epsilon \eta] - J[q] \approx \epsilon \int \frac{\delta J}{\delta q(x)} \eta(x) \, dx\] <p>Here, $\eta(x)$ is an arbitrary test function, similar to the role of $\Delta x$ in function derivative.</p> <p><strong>Solution</strong></p> <p>It’s easy to calculate the functional derivative of $J$ in the simple form defined above. First, adding some perturbation to $J$:</p> \[J[q + \epsilon \eta] = \int_a^b F(q(x) + \epsilon \eta(x), x) \, dx\] <p>Applying Taylor expansion to $F$ around $x$:</p> \[J[q + \epsilon \eta] = \int_a^b \left[F(q(x), x) \, + \frac{\partial F}{\partial q} \epsilon \eta(x)\right] dx\] \[J[q + \epsilon \eta] - J[q] = \epsilon \int_a^b \frac{\partial F}{\partial q} \eta(x) dx\] <p>Looking back at the definition of functional derivative, we’ll find that $\frac{\delta J}{\delta q(x)}=\frac{\partial F}{\partial q}$</p> <p><strong>Application</strong></p> <p>The conclusion above can be applied to solve the problem of entropy-regularized expectation optimization:</p> \[\mathcal{F}(q) = \int q(x) f(x) \, dx - \lambda \int q(x) \log q(x) \, dx\] <p>The functional derivative of $ \mathcal{F}(q) $ with respect to $ q(x) $ involves:</p> <ul> <li> <p><strong>Expectation Term</strong>: \(\frac{\delta}{\delta q(x)} \left( \int q(x) f(x) \, dx \right) = f(x)\)</p> </li> <li> <p><strong>Entropy Term</strong>: \(\frac{\delta}{\delta q(x)} \left(- \lambda \int q(x) \log q(x) \, dx \right) = -\lambda (\log q(x) + 1)\)</p> </li> </ul> <p>Combining these:</p> \[\frac{\delta \mathcal{F}(q)}{\delta q(x)} = f(x) - \lambda (\log q(x) + 1)\] <p>Setting this to zero to find the optimal $ q(x)$:</p> \[f(x) - \lambda (\log q(x) + 1) = 0\] <p>Solving for $q(x)$:</p> \[q(x) = \exp\left( \frac{f(x)}{\lambda} - 1 \right)\] <p>Since we haven’t put any constraint to $q$, the resulting $q$ may not be a probability distribution. However, we know that adding any constant $C$ to $f(x)$ will not affect the solution $q(x)$, and there must exist a $C$ that makes the solution $q(x)$ integrate to $1$. Then, the solution $q(x)$ to the new problem will still satisfy:</p> \[q(x) \propto \exp\left( \frac{f(x)}{\lambda} \right)\] <h2 id="formulation">Formulation</h2> <p>The objective of maximum entropy RL is:</p> \[\pi_{\text {MaxEnt }}^*=\arg \max _\pi \sum_t \mathbb{E}_{\left(\mathbf{s}_t, \mathbf{a}_t\right) \sim \rho_\pi}\left[r\left(\mathbf{s}_t, \mathbf{a}_t\right)+\alpha \mathcal{H}\left(\pi\left(\cdot \mid \mathbf{s}_t\right)\right)\right]\] <p>Note this objective not only maximizes entropy <strong>at the current time step</strong>, but also encourages the policy to reach states where they will have high entropy <strong>in the future</strong>.</p> <p>We can first define soft Q-function and soft value function by incorporating entropy into standard Q-function and value function:</p> \[\begin{aligned} &amp; Q_{\text {soft }}^{\pi}\left(\mathbf{s}_t, \mathbf{a}_t\right)=r_t+ \mathbb{E}_{\left(\mathbf{s}_{t+1}, \ldots\right) \sim \rho_\pi}\left[\sum_{l=1}^{\infty} \gamma^l\left(r_{t+l}+\alpha \mathcal{H}\left(\pi\left(\cdot \mid \mathbf{s}_{t+l}\right)\right)\right)\right] \end{aligned}\] \[V_{\mathrm{soft}}^\pi\left(\mathbf{s}_t\right)=\mathbb{E}_{a'\sim \pi(\mathbf{s}_t)} \left[Q_{\mathrm{soft}}^\pi\left(\mathbf{s}_t, \mathbf{a}^{\prime}\right) + \alpha \mathcal{H}\left(\pi\left(\cdot \mid \mathbf{s}_{t+l}\right)\right)\right]\] <p>Note that the above definition of soft value function has been shortened by using $Q$.</p> <p>To improve a policy $\pi$, we can optimize the soft value function above:</p> \[\hat\pi(\mathbf{s}_t)\propto \exp\left(\frac{1}{\alpha}Q_{\mathrm{soft}}^\pi\left(\mathbf{s}_t, \mathbf{a}^{\prime}\right)\right)\] <p>That’s what we have proved with functional derivative.</p> <p>So, the optimal soft policy has to satisfy the form below, because otherwise it can be further improved in this way.</p> \[\pi^*(\mathbf{s}_t)\propto \exp\left(\frac{1}{\alpha}Q_{\mathrm{soft}}^*\left(\mathbf{s}_t, \mathbf{a}^{\prime}\right)\right)\] <p>We can directly plug it into the definition of soft value function, and get soft Bellman equation (Theorem 2 in <a href="https://arxiv.org/abs/1702.08165">Soft Q-Learning</a>):</p> \[V_{\mathrm{soft}}^*\left(\mathbf{s}_t\right)=\alpha \log \int_{\mathcal{A}} \exp \left(\frac{1}{\alpha} Q_{\mathrm{soft}}^*\left(\mathbf{s}_t, \mathbf{a}^{\prime}\right)\right) d \mathbf{a}^{\prime}\] <p>When $\alpha=0$, it becomes standard Bellman equation, where the $\log \int\exp$ operator becomes $\max$.</p> <p>The integration in the equation above is the normalizing constant of policy, so we can express the constant with soft value function:</p> \[\int_{\mathcal{A}} \exp \left(\frac{1}{\alpha} Q_{\mathrm{soft}}^*\left(\mathbf{s}_t, \mathbf{a}^{\prime}\right)\right) d \mathbf{a}^{\prime}=\exp \frac{1}{\alpha}V_{\mathrm{soft}}^*\left(\mathbf{s}_t\right)\] <p>Then, the optimal soft policy can be written as below (Theorem 1 in <a href="https://arxiv.org/abs/1702.08165">Soft Q-Learning</a>):</p> \[\pi_{\text {MaxEnt }}^*\left(\mathbf{a}_t \mid \mathbf{s}_t\right)=\exp \left(\frac{1}{\alpha}\left(Q_{\mathrm{soft}}^*\left(\mathbf{s}_t, \mathbf{a}_t\right)-V_{\mathrm{soft}}^*\left(\mathbf{s}_t\right)\right)\right)\] <h2 id="soft-q-iteration">Soft Q-Iteration</h2> <p>Soft Q-Learning is based on fixed-point iteration that resembles Q-iteration.</p> \[\begin{aligned} Q_{\text {soft }}\left(\mathbf{s}_t, \mathbf{a}_t\right) &amp; \leftarrow r_t+\gamma \mathbb{E}_{\mathbf{s}_{t+1} \sim p_{\mathbf{s}}}\left[V_{\text {soft }}\left(\mathbf{s}_{t+1}\right)\right], \forall \mathbf{s}_t, \mathbf{a}_t \\ V_{\text {soft }}\left(\mathbf{s}_t\right) &amp; \leftarrow \alpha \log \int_{\mathcal{A}} \exp \left(\frac{1}{\alpha} Q_{\text {soft }}\left(\mathbf{s}_t, \mathbf{a}^{\prime}\right)\right) d \mathbf{a}^{\prime}, \forall \mathbf{s}_t \end{aligned}\] <p>The iteration will converge to \(Q^*_{\text{soft}}\) and \(V^*_{\text{soft}}\) (Theorem 3 in <a href="https://arxiv.org/abs/1702.08165">Soft Q-Learning</a>). Apparently, the policy can be monotonically improved in this way, and the process would only stop when the soft Bellman equation is satisfied everywhere.</p> <h3 id="learning-soft-q-function">Learning soft Q function</h3> <p>Soft Q Learning parametrize soft Q function $Q_{\text{soft}}^\theta(s,a)$ and represent soft value function with it. At each step of soft Q-iteration:</p> <ul> <li> <p>First estimate $V^\theta_{\text{soft}}(s)$ with importance sampling using any policy $q$: \(V_{\mathrm{soft}}^\theta\left(\mathbf{s}_t\right)=\alpha \log \mathbb{E}_{q_{\mathbf{a}^{\prime}}}\left[\frac{\exp \left(\frac{1}{\alpha} Q_{\mathrm{soft}}^\theta\left(\mathbf{s}_t, \mathbf{a}^{\prime}\right)\right)}{q_{\mathbf{a}^{\prime}}\left(\mathbf{a}^{\prime}\right)}\right]\)</p> </li> <li> <p>Then we could calculate the <strong>target Q function</strong> using the value function of next states. \(\hat{Q}_{\mathrm{soft}}^{\bar{\theta}}\left(\mathbf{s}_t, \mathbf{a}_t\right)=r_t+\gamma \mathbb{E}_{\mathbf{s}_{t+1} \sim p_{\mathrm{s}}}\left[V_{\mathrm{soft}}^{\bar{\theta}}\left(\mathbf{s}_{t+1}\right)\right]\)</p> </li> <li> <p>Optimize the soft Q function towards the target Q function: \(J_Q(\theta)=\mathbb{E}_{\mathbf{s}_t \sim q_{s_t}, \mathbf{a}_t \sim q_{a_t}}\left[\frac{1}{2}\left(\hat{Q}_{\text {soft }}^{\bar{\theta}}\left(\mathbf{s}_t, \mathbf{a}_t\right)-Q_{\text {soft }}^\theta\left(\mathbf{s}_t, \mathbf{a}_t\right)\right)^2\right]\)</p> </li> </ul> <h3 id="sampling-from-soft-policy">Sampling from soft policy</h3> <p>The next challenge is to sample from the policy, which is non-trivial because it’s an energy-based model of the soft Q function, with an unknwon normalizing constant.</p> <p>The <a href="https://arxiv.org/abs/1702.08165">paper</a> introduces <a href="https://arxiv.org/abs/1608.04471">SVGD</a> to sample from the policy. This involves:</p> <ul> <li> <p>Parametrize a policy network $f^\phi\left(\xi ; \mathbf{s}_t\right)$ that maps noise sampple $\xi$ from Gaussian to an action.</p> </li> <li> <p>Minimizing the KL divergence of $\pi^\phi$ to the energy-based policy of Q functions: First randomly ample a set of actions, and compute the most greedy directions for them to minimize the KL divergence, then backpropagate the gradient into the policy network.</p> </li> </ul> <p>Because this part is relatively apart from the main formulation of maximum entropy RL, we skip the details of SVGD in this blog.</p> <hr/> <p><strong>Acknowledgement</strong>: Thanks to <a href="https://sites.google.com/view/yingyulin">Yingyu Lin</a> for disucssion and proofreading.</p>]]></content><author><name></name></author><category term="reinforcement-learning,"/><category term="note"/><summary type="html"><![CDATA[Notes on maximum entropy RL and an introduction to soft Q-learning]]></summary></entry><entry><title type="html">DSC243 Note (2): From Local Convergence to Global Convergence</title><link href="https://ber666.github.io/blog/2024/Optimization-2/" rel="alternate" type="text/html" title="DSC243 Note (2): From Local Convergence to Global Convergence"/><published>2024-04-22T13:56:00+00:00</published><updated>2024-04-22T13:56:00+00:00</updated><id>https://ber666.github.io/blog/2024/Optimization-2</id><content type="html" xml:base="https://ber666.github.io/blog/2024/Optimization-2/"><![CDATA[<h2 id="probabilistic-definition-of-solution">Probabilistic Definition of Solution</h2> <p>What is a solution?</p> <ul> <li> <p>Problem: $min_{x\in A}f(x)$</p> </li> <li> <p>Solution: $|f(x)-f(x^*)|\le \epsilon$, with prob $1-\delta$</p> </li> </ul> <p>This is a <strong>probabilistic</strong> definition, because we usually use stochastic algorithm. As the algorithm runs longer, it <strong>asymptotically</strong> converges ($\epsilon$ and $\delta$ go to 0).</p> <h2 id="local-solution">Local Solution</h2> <p>Finding a global optimum is difficult (as discussed in the last lecture), we can talk about local solutions:</p> <p>Imagine we are using gradient descent, it will stop when it reaches a local optimum.</p> <p>First order stationary point (FOSP): $\nabla f(x)=0$</p> <p>It doesn’t specify the <strong>stability</strong> of the solution.</p> <p>Second order stationary point: $\nabla^2 f(x)\succ 0$ and $\nabla f(x)=0$. (The Hessian matrix is positive definite + first order stationary.)</p> <p>A more popular definition of second-order stationary point actually only requires positive semi-definite, but you cannot guarentee it’s stable then.</p> <p>Around FOSP, we can apply Taylor series (approximated to second order):</p> \[f(x)\approx f(x^*) + &lt;\nabla f(x^*), x-x^*&gt;+\frac{1}{2}(x-x^*)^T\nabla^2f(x^*)(x-x^*) = \phi(x)\] <p>The gradient is zero, so we only need to worry about the second-order. How would gradient descent behave?</p> <p>$H:=\nabla^2 f(x)$, $h$ is the learning rate.</p> \[x_{t+1}=x_t - h\nabla \phi(x)=x_t - hH(x_t - x^*)\] <blockquote> <p>Quick review: $ \frac{1}{2}x^TAx = \frac{1}{2}\sum_{i\ne j}2a_{ij}x_ix_j + \frac{1}{2}\sum_{i}a_{ii}x_i^2$. Looking at the gradient for $a_i$, it’s $\sum_ja_{ij}x_j$, which is the $i$ th element of $Ax$.</p> </blockquote> \[x_{t+1}-x^* = (I-hH)(x_t - x^*)\] <p>$N$ is the optimization step</p> <p>$H=u\Lambda u^T, \Lambda$ is a diagnoal matrix composed of all eigen values.</p> \[x_T - x^* = (I-hH)^N(x_0-x^*)=u(I-h\Lambda)^Nu^T(x_0-x^*)\] <ul> <li>Case 1: If one of the eigen values is negative, $I-h\lambda_i$ is larger than 1, so it would explode. This is a saddle point.</li> <li>Case 2: $\lambda_1 &gt; … \lambda_d&gt; 0$.</li> </ul> <blockquote> <p>Quick review: The norm of a matrix: \(||A||_p=\max_{||x||_p&lt;1} ||Ax||{_p}\). Geometrically speaking, one can imagine a p-norm unit ball, then apply the linear map to the ball. It would end up becoming a distorted convex shape and p-norm measures the longest “radius” of the distorted convex shape. According to the definition, we have $||Ax||\le ||A||\ ||x||$</p> </blockquote> \[\begin{align*} ||x_{T}-x^*||_2 &amp;= ||u(I-h\Lambda)^N u^T (x_0-x^*)|| \\ &amp;\le ||u(I-h\Lambda)^N u^T ||_2 \ ||(x_0-x^*)||_2 \end{align*}\] <p>We need to have $h\le \frac{1}{\lambda}$, otherwise it will explode. (*) Choose it to be $\lambda_1$, the max eigen value. So, in the last direction is the slowest direction, because $1-\frac{\lambda_d}{\lambda_1}$ is close to one.</p> \[||x_T-x^*||_2 \le (I-\frac{\lambda_d}{\lambda_1})^T||x_0 - x^*||_2 \le \epsilon\] <p>It takes $T=\frac{\lambda_1}{\lambda_d}\log \frac{||x_0 - x^*||_2}{\epsilon}$ to converge. Here, $\frac{\lambda_1}{\lambda_d}$ is the condition number.</p> <p>(*) Note that, only the first-order information is visible to GD. We make use of the Hessian matrix only to find what’s the best GD can achieve.</p> <h2 id="towards-global-solution">Towards global solution</h2> <p>We need stronger regularization.</p> <p><strong>Convexity</strong></p> <ul> <li> <p>Second-order condition: \(\nabla^2f(x)\succeq 0\)</p> </li> <li> <p>First-order condition: Predicting function value with tangent, the actual function is always above the projection.</p> \[&lt;\nabla f(x), y-x&gt; \le f(y) - f(x)\] <p>Local information tells a lot about global information: You can ignore a direction if the gradient is larger than 0, because you know it will never decrease.</p> </li> <li> <p>Zero-order condition: The intersect line is always above the actual functions</p> \[f(\lambda x + (1-\lambda) y) \le \lambda f(x) + (1-\lambda)f(y)\] </li> </ul> <p><strong>Proof</strong></p> <p>Hint: Think about the geometry. Increasing order need taking derivation, otherwise integration.</p> <ul> <li> <p>0 -&gt; 1:</p> \[f(y+\lambda(x-y)) \le \lambda f(x) + (1-\lambda)f(y)\] <p>Taking derivative of $\lambda$:</p> <blockquote> <p>Quick review: Think of the limit when $\lambda \rightarrow 0$, and ignore smaller items in the Taylor expansion.</p> </blockquote> \[&lt;\nabla f(y+\lambda(x-y)), (x-y)&gt; \le f(x) - f(y)\] <p>Setting $\lambda =0$:</p> \[&lt;\nabla f(y), x-y&gt; \le f(x) - f(y)\] </li> <li> <p>1 -&gt; 2:</p> <p><strong>(a)</strong> Starting with the simple case where $d=1$.</p> \[f(y) \ge f(x) + f'(x) (y-x)\] \[f(x) \ge f(y) + f'(y) (x-y)\] <p>From these two inequation, we can get:</p> \[f'(y)(x-y) \le f(x)-f(y) \le f'(x)(x-y)\] <p>Assuming $x-y&gt;0$,</p> \[\frac{f'(x)-f'(y)}{x-y}\ge 0\] <p><strong>(b)</strong> For any dimensions, we want to make it into 1-d:</p> \[g(\alpha) = f(x+\alpha v)\] <p>We can show it’s also a convex function by expanding it. So, applying the $1-d$ conclusion, we have:</p> \[g''(\alpha)\ge 0\] \[v^T\nabla f(x+\alpha v)v \ge 0\] <p>Taking $\alpha=0$, we get the definition of semi-positive matrix.</p> </li> </ul> <p><strong>Strongly convex</strong></p> <blockquote> <p>Intuition: $x^2$ is the standard convex function. If a function is “more convex” than $mx^2$…</p> </blockquote> <p>Definition of $m$-strong convex: \(f(x) - \frac{m}{2} ||x||^2 \text{ is convex.}\)</p> <ul> <li>Second-order: \(\nabla^2f(x)\succeq m\cdot I\) \(\lambda_{\min}(\nabla^2f(x))\ge m\)</li> <li>First-order: \(f(y)-f(x)\ge &lt;\nabla f(x), y-x&gt; + \frac{m}{2} \|\|y-x\|\|^2\)</li> <li>Zero-order: Skipped. (simply apply the definition)</li> </ul> <p>In the next class, we will show how to use these conditions to derive global convergence.</p>]]></content><author><name></name></author><category term="optimization,"/><category term="note"/><summary type="html"><![CDATA[Notes of DSC 243: Advanced Optimization]]></summary></entry></feed>