---
layout: post
title: "Maximum Entropy RL (2): Connection to Actor-Critic Algorithm"
date: 2024-12-09 09:56:00-0400
description: "Notes on the connection between maximum entropy RL and actor-critic algorithm"
tags: "reinforcement-learning, note"
categories:
giscus_comments: false
related_posts: false
related_publications: 
---

Although the derivation of Soft Q-Learning (SQL, see our previous [post](blog/2024/soft-RL-01/)) relies on the Soft Bellman Equation and is similar to the derivation of Q-Learning (which relies on the Bellman Equation), the optimal policy obtained from SQL is a stochastic policy, rather than a deterministic policy that always maximizes the Q-function, as is the case with Q-Learning. How should we understand SQL in comparison to policy-based methods, which directly learn a policy (usually also outputting a probability distribution over actions)?

### Parametrization of SQL

Before discussing the connection to actor-critic algorithm, we first introduce other variants of SQL.

The key of SQL is **optimizing the soft Bellman consistency**. The SQL paper parametrizes the soft Q-function and learns it with soft Q-iteration. Alternatively, one can also parametrize other components (e.g., the policy, and/or the value function). [Path Consistency Learning](https://arxiv.org/abs/1702.08892) parametrize the value function $V_\phi$ and policy $\pi_\theta$, and directly optimizing the Bellman consistency:

$$
C\left(s_{i: i+1}, \theta, \phi\right)=-V_\phi\left(s_i\right)+V_\phi\left(s_{i+1}\right)+r\left(s_{i}, a_{i}\right)-\alpha \log \pi_\theta\left(a_{i} \mid s_{i}\right)\\

J_\text{PCL}(\theta, \phi) = \mathbb{E}_{s_{i, i+1}\sim D} \left[\frac{1}{2}C\left(s_{i: i+1}, \theta, \phi\right)\right]^2
$$

Besides parametrization, another difference is that SQL stops the gradient of the target Q-function, which is regarded to have better numerical properties (which has been widely discussed in deep Q-learning). In PCL, there is no such operation, but the overall objective is the same, that is to optimize soft Bellman consistency.

### Connecting to Actor-Critic

When we parametrize policy and value function, it's easier to build a connection to the actor-critic algorithm. First, let's compare the gradient to $\theta$.
$$
\begin{align*}
\nabla_\theta J_\text{PCL} &= \mathbb{E}_{s_{i, i+1}\sim D} \left[C\left(s_{i: i+1}, \theta, \phi\right)\nabla_\theta C\left(s_{i: i+1}, \theta, \phi\right)\right]\\
&=\mathbb{E}_{s_{i, i+1}\sim D}\left[\alpha (V_\phi\left(s_i\right) - V_\phi\left(s_{i+1}\right) - r\left(s_{i}, a_{i}\right) + \alpha \log \pi_\theta\left(a_{i} \mid s_{i}\right))\nabla_\theta \log \pi_\theta (a_i \mid s_i) \right]
\end{align*}
$$

In AC, the objective is: 


$$
A\left(s_{i: i+1}, \theta, \phi\right)=-V_\phi\left(s_i\right)+V_\phi\left(s_{i+1}\right)+r\left(s_{i}, a_{i}\right)\\

J_\text{AC}(\theta) = \mathbb{E}_{s_{i, i+1}\sim \pi_\theta} A\left(s_{i: i+1}, \theta, \phi\right)

$$

and the gradient to $\theta$ is:

$$
\begin{align*}
\nabla_\theta J_\text{AC} &= \mathbb{E}_{s_{i, i+1}\sim \pi_\theta} \left[\nabla_\theta\log \pi_\theta(a_i \mid s_i) A\left(s_{i: i+1}, \theta, \phi\right)\right]\\
&= \mathbb{E}_{s_{i, i+1}\sim \pi_\theta} \left[\nabla_\theta\log \pi_\theta(a_i \mid s_i) \left(-V_\phi\left(s_i\right)+V_\phi\left(s_{i+1}\right)+r\left(s_{i}, a_{i}\right)\right)\right]

\end{align*}
$$
