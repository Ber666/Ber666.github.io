---
layout: post
title: "Paper Digest 2024-07-08: Multi-token Prediction"
date: 2024-07-08 09:56:00-0400
description: "Paper Digest"
tags: "large-language-model, diffusion-model"
categories:
giscus_comments: false
related_posts: false
related_publications: 
---

## Better & Faster Large Language Models via Multi-token Prediction
- Motivation: Inefficiency of the next-token prediction objective. Teacher forcing with next-token prediction latches on local patterns and overlooks “hard” decisions. LLMs call for orders of magnitude more data than human children to arrive at the same level of fluency.
- Core idea: Add multiple heads to predict the next-k-token (Figure 1). This can be seen as an auxiliary training task, which boosts LLMs' reasoning ability with next (one) token prediction, or used in speculative decoding similar to Medusa (but with a simpler architecture).
- Main results:
    - ...
- Important details:
    - Back-propagate different heads sequentially to avoid GPU memory peak (Figure 2).
    - Beautiful figures. May refer to it later.