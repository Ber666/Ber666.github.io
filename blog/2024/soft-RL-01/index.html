<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Maximum Entropy RL (1): Soft Q-Learning | Shibo Hao</title> <meta name="author" content="Shibo Hao"> <meta name="description" content="Notes on maximum entropy RL and an introduction to soft Q-learning"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%A1&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ber666.github.io/blog/2024/soft-RL-01/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Shibo </span>Hao</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Maximum Entropy RL (1): Soft Q-Learning</h1> <p class="post-meta">August 4, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/reinforcement-learning"> <i class="fas fa-hashtag fa-sm"></i> reinforcement-learning,</a>   <a href="/blog/tag/note"> <i class="fas fa-hashtag fa-sm"></i> note</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Most deep reinforcement learning (RL) is based on the deterministic notion of optimality, where the optimal solution is always a deterministic policy (at least under full observability). However, a stochastic policy may be preferred due to:</p> <ul> <li> <strong>Better exploration</strong>: Strike a balance between exploration and exploitation, in contrast to heuristic exploration methods like $\epsilon$-greedy.</li> <li> <strong>Robustness and better generalization</strong>: A stochastic policy is more robust to adversarial perturbations and serves as a better initialization for learning new skills.</li> </ul> <p><strong>Maximum entropy RL</strong> attempts to optimize the entropy of policy in addition to the expected reward, which encourages the policy to be more stochastic. The new learning objective also has some interesting properties and leads to useful algorithms which bridge traditional value and policy based methods (<a href="https://arxiv.org/abs/1702.08165" rel="external nofollow noopener" target="_blank">Soft Q-Learning</a>, <a href="https://arxiv.org/abs/1702.08892" rel="external nofollow noopener" target="_blank">Path Consistency Learning</a>, <a href="https://arxiv.org/abs/1801.01290" rel="external nofollow noopener" target="_blank">Soft Actor-Critic</a>, etc.)</p> <h2 id="functional-derivative">Functional Derivative</h2> <p>In this section, we provide a quick introduction of functional derivative, which is crucial to prove some key theorems in maximum entropy RL.</p> <p><strong>Definition</strong></p> <p>A functional $J(q)$ is a mapping from function $q(x)$ to a real value. Here, we focus on a simple case of functional, which can be expressed as an integration of a function of $q(x)$:</p> \[J[q] = \int_a^b F(q(x), x) \, dx\] <p>To measure <strong>“how a small change in $q(x)$ would affect $J(q)$”</strong>, we define the functinoal derivative $\frac{\delta J}{\delta q(x)}$ as follows:</p> \[J[q + \epsilon \eta] - J[q] \approx \epsilon \int \frac{\delta J}{\delta q(x)} \eta(x) \, dx\] <p>Here, $\eta(x)$ is an arbitrary test function, similar to the role of $\Delta x$ in function derivative.</p> <p><strong>Solution</strong></p> <p>It’s easy to calculate the functional derivative of $J$ in the simple form defined above. First, adding some perturbation to $J$:</p> \[J[q + \epsilon \eta] = \int_a^b F(q(x) + \epsilon \eta(x), x) \, dx\] <p>Applying Taylor expansion to $F$ around $x$:</p> \[J[q + \epsilon \eta] = \int_a^b \left[F(q(x), x) \, + \frac{\partial F}{\partial q} \epsilon \eta(x)\right] dx\] \[J[q + \epsilon \eta] - J[q] = \epsilon \int_a^b \frac{\partial F}{\partial q} \eta(x) dx\] <p>Looking back at the definition of functional derivative, we’ll find that $\frac{\delta J}{\delta q(x)}=\frac{\partial F}{\partial q}$</p> <p><strong>Application</strong></p> <p>The conclusion above can be applied to solve the problem of entropy-regularized expectation optimization:</p> \[\mathcal{F}(q) = \int q(x) f(x) \, dx - \lambda \int q(x) \log q(x) \, dx\] <p>The functional derivative of $ \mathcal{F}(q) $ with respect to $ q(x) $ involves:</p> <ul> <li> <p><strong>Expectation Term</strong>: \(\frac{\delta}{\delta q(x)} \left( \int q(x) f(x) \, dx \right) = f(x)\)</p> </li> <li> <p><strong>Entropy Term</strong>: \(\frac{\delta}{\delta q(x)} \left(- \lambda \int q(x) \log q(x) \, dx \right) = -\lambda (\log q(x) + 1)\)</p> </li> </ul> <p>Combining these:</p> \[\frac{\delta \mathcal{F}(q)}{\delta q(x)} = f(x) - \lambda (\log q(x) + 1)\] <p>Setting this to zero to find the optimal $ q(x)$:</p> \[f(x) - \lambda (\log q(x) + 1) = 0\] <p>Solving for $q(x)$:</p> \[q(x) = \exp\left( \frac{f(x)}{\lambda} - 1 \right)\] <p>Since we haven’t put any constraint to $q$, the resulting $q$ may not be a probability distribution. However, we know that adding any constant $C$ to $f(x)$ will not affect the solution $q(x)$, and there must exist a $C$ that makes the solution $q(x)$ integrate to $1$. Then, the solution $q(x)$ to the new problem will still satisfy:</p> \[q(x) \propto \exp\left( \frac{f(x)}{\lambda} \right)\] <h2 id="formulation">Formulation</h2> <p>The objective of maximum entropy RL is:</p> \[\pi_{\text {MaxEnt }}^*=\arg \max _\pi \sum_t \mathbb{E}_{\left(\mathbf{s}_t, \mathbf{a}_t\right) \sim \rho_\pi}\left[r\left(\mathbf{s}_t, \mathbf{a}_t\right)+\alpha \mathcal{H}\left(\pi\left(\cdot \mid \mathbf{s}_t\right)\right)\right]\] <p>Note this objective not only maximizes entropy <strong>at the current time step</strong>, but also encourages the policy to reach states where they will have high entropy <strong>in the future</strong>.</p> <p>We can first define soft Q-function and soft value function by incorporating entropy into standard Q-function and value function:</p> \[\begin{aligned} &amp; Q_{\text {soft }}^{\pi}\left(\mathbf{s}_t, \mathbf{a}_t\right)=r_t+ \mathbb{E}_{\left(\mathbf{s}_{t+1}, \ldots\right) \sim \rho_\pi}\left[\sum_{l=1}^{\infty} \gamma^l\left(r_{t+l}+\alpha \mathcal{H}\left(\pi\left(\cdot \mid \mathbf{s}_{t+l}\right)\right)\right)\right] \end{aligned}\] \[V_{\mathrm{soft}}^\pi\left(\mathbf{s}_t\right)=\mathbb{E}_{a'\sim \pi(\mathbf{s}_t)} \left[Q_{\mathrm{soft}}^\pi\left(\mathbf{s}_t, \mathbf{a}^{\prime}\right) + \alpha \mathcal{H}\left(\pi\left(\cdot \mid \mathbf{s}_{t+l}\right)\right)\right]\] <p>Note that the above definition of soft value function has been shortened by using $Q$.</p> <p>To improve a policy $\pi$, we can optimize the soft value function above:</p> \[\hat\pi(\mathbf{s}_t)\propto \exp\left(\frac{1}{\alpha}Q_{\mathrm{soft}}^\pi\left(\mathbf{s}_t, \mathbf{a}^{\prime}\right)\right)\] <p>That’s what we have proved with functional derivative.</p> <p>So, the optimal soft policy has to satisfy the form below, because otherwise it can be further improved in this way.</p> \[\pi^*(\mathbf{s}_t)\propto \exp\left(\frac{1}{\alpha}Q_{\mathrm{soft}}^*\left(\mathbf{s}_t, \mathbf{a}^{\prime}\right)\right)\] <p>We can directly plug it into the definition of soft value function, and get soft Bellman equation (Theorem 2 in <a href="https://arxiv.org/abs/1702.08165" rel="external nofollow noopener" target="_blank">Soft Q-Learning</a>):</p> \[V_{\mathrm{soft}}^*\left(\mathbf{s}_t\right)=\alpha \log \int_{\mathcal{A}} \exp \left(\frac{1}{\alpha} Q_{\mathrm{soft}}^*\left(\mathbf{s}_t, \mathbf{a}^{\prime}\right)\right) d \mathbf{a}^{\prime}\] <p>When $\alpha=0$, it becomes standard Bellman equation, where the $\log \int\exp$ operator becomes $\max$.</p> <p>The integration in the equation above is the normalizing constant of policy, so we can express the constant with soft value function:</p> \[\int_{\mathcal{A}} \exp \left(\frac{1}{\alpha} Q_{\mathrm{soft}}^*\left(\mathbf{s}_t, \mathbf{a}^{\prime}\right)\right) d \mathbf{a}^{\prime}=\exp \frac{1}{\alpha}V_{\mathrm{soft}}^*\left(\mathbf{s}_t\right)\] <p>Then, the optimal soft policy can be written as below (Theorem 1 in <a href="https://arxiv.org/abs/1702.08165" rel="external nofollow noopener" target="_blank">Soft Q-Learning</a>):</p> \[\pi_{\text {MaxEnt }}^*\left(\mathbf{a}_t \mid \mathbf{s}_t\right)=\exp \left(\frac{1}{\alpha}\left(Q_{\mathrm{soft}}^*\left(\mathbf{s}_t, \mathbf{a}_t\right)-V_{\mathrm{soft}}^*\left(\mathbf{s}_t\right)\right)\right)\] <h2 id="soft-q-iteration">Soft Q-Iteration</h2> <p>Soft Q-Learning is based on fixed-point iteration that resembles Q-iteration.</p> \[\begin{aligned} Q_{\text {soft }}\left(\mathbf{s}_t, \mathbf{a}_t\right) &amp; \leftarrow r_t+\gamma \mathbb{E}_{\mathbf{s}_{t+1} \sim p_{\mathbf{s}}}\left[V_{\text {soft }}\left(\mathbf{s}_{t+1}\right)\right], \forall \mathbf{s}_t, \mathbf{a}_t \\ V_{\text {soft }}\left(\mathbf{s}_t\right) &amp; \leftarrow \alpha \log \int_{\mathcal{A}} \exp \left(\frac{1}{\alpha} Q_{\text {soft }}\left(\mathbf{s}_t, \mathbf{a}^{\prime}\right)\right) d \mathbf{a}^{\prime}, \forall \mathbf{s}_t \end{aligned}\] <p>The iteration will converge to \(Q^*_{\text{soft}}\) and \(V^*_{\text{soft}}\) (Theorem 3 in <a href="https://arxiv.org/abs/1702.08165" rel="external nofollow noopener" target="_blank">Soft Q-Learning</a>). Apparently, the policy can be monotonically improved in this way, and the process would only stop when the soft Bellman equation is satisfied everywhere.</p> <h3 id="learning-soft-q-function">Learning soft Q function</h3> <p>Soft Q Learning parametrize soft Q function $Q_{\text{soft}}^\theta(s,a)$ and represent soft value function with it. At each step of soft Q-iteration:</p> <ul> <li> <p>First estimate $V^\theta_{\text{soft}}(s)$ with importance sampling using any policy $q$. Note that if the action space is finite, importance sampling is unnecessary, as we could directly enumerate all possible actions. \(V_{\mathrm{soft}}^\theta\left(\mathbf{s}_t\right)=\alpha \log \mathbb{E}_{q_{\mathbf{a}^{\prime}}}\left[\frac{\exp \left(\frac{1}{\alpha} Q_{\mathrm{soft}}^\theta\left(\mathbf{s}_t, \mathbf{a}^{\prime}\right)\right)}{q_{\mathbf{a}^{\prime}}\left(\mathbf{a}^{\prime}\right)}\right]\)</p> </li> <li> <p>Then we could calculate the <strong>target Q function</strong> using the value function of next states. \(\hat{Q}_{\mathrm{soft}}^{\bar{\theta}}\left(\mathbf{s}_t, \mathbf{a}_t\right)=r_t+\gamma \mathbb{E}_{\mathbf{s}_{t+1} \sim p_{\mathrm{s}}}\left[V_{\mathrm{soft}}^{\bar{\theta}}\left(\mathbf{s}_{t+1}\right)\right]\)</p> </li> <li> <p>Optimize the soft Q function towards the target Q function: \(J_Q(\theta)=\mathbb{E}_{\mathbf{s}_t \sim q_{s_t}, \mathbf{a}_t \sim q_{a_t}}\left[\frac{1}{2}\left(\hat{Q}_{\text {soft }}^{\bar{\theta}}\left(\mathbf{s}_t, \mathbf{a}_t\right)-Q_{\text {soft }}^\theta\left(\mathbf{s}_t, \mathbf{a}_t\right)\right)^2\right]\)</p> </li> </ul> <h3 id="sampling-from-soft-policy">Sampling from soft policy</h3> <p>The next challenge is to sample from the policy, which is non-trivial because it’s an energy-based model of the soft Q function, with an unknwon normalizing constant.</p> <p>The <a href="https://arxiv.org/abs/1702.08165" rel="external nofollow noopener" target="_blank">paper</a> introduces <a href="https://arxiv.org/abs/1608.04471" rel="external nofollow noopener" target="_blank">SVGD</a> to sample from the policy. This involves:</p> <ul> <li> <p>Parametrize a policy network $f^\phi\left(\xi ; \mathbf{s}_t\right)$ that maps noise sampple $\xi$ from Gaussian to an action.</p> </li> <li> <p>Minimizing the KL divergence of $\pi^\phi$ to the energy-based policy of Q functions: First randomly ample a set of actions, and compute the most greedy directions for them to minimize the KL divergence, then backpropagate the gradient into the policy network.</p> </li> </ul> <p>Because this part is relatively apart from the main formulation of maximum entropy RL, we skip the details of SVGD in this blog.</p> <hr> <p><strong>Acknowledgement</strong>: Thanks to <a href="https://sites.google.com/view/yingyulin" rel="external nofollow noopener" target="_blank">Yingyu Lin</a> for disucssion and proofreading.</p> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Shibo Hao. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 01, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams",inlineMath:[["$","$"],["\\(","\\)"]]}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>